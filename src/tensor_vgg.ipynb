{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from numpy import nan\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = IMG_WIDTH, IMG_HEIGHT\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    shape_ord = (3, img_rows, img_cols)\n",
    "else:  # channel_last\n",
    "    shape_ord = (img_rows, img_cols, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras.layers import Input\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "vgg16 = vgg16.VGG16(weights='imagenet', include_top=False, input_tensor=Input((224, 224, 3)))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in vgg16.layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten(input_shape=vgg16.output.shape)(vgg16.output)\n",
    "# x = Dense(4096, activation='relu', name='fully_connected_1')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(2048, activation='relu', name='fully_connected_1')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fully_connected_2')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(4, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=vgg16.input, outputs=predictions)\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fully_connected_1 (Dense)    (None, 2048)              51382272  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fully_connected_2 (Dense)    (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 68,203,332\n",
      "Trainable params: 53,486,596\n",
      "Non-trainable params: 14,716,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        zoom_range=0.2,\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2491 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../data/train',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32, #changed from 1, change back if needed\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 846 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '../data/validation',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 377 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        '../data/test',\n",
    "        target_size=(224,224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 78/118 [==================>...........] - ETA: 43s - loss: 1.3876 - acc: 0.4855 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 2/20\n",
      " 78/118 [==================>...........] - ETA: 37s - loss: 1.0630 - acc: 0.5924 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 3/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.9912 - acc: 0.6267 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 4/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.9001 - acc: 0.6624 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 5/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.8147 - acc: 0.6885 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 6/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.7761 - acc: 0.7072 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 7/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.6993 - acc: 0.7461 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 8/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.6638 - acc: 0.7477 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 9/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.6429 - acc: 0.7527 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 10/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.5532 - acc: 0.7887 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 11/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.5492 - acc: 0.7929 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 12/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.5284 - acc: 0.8058 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 13/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.5142 - acc: 0.8044 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 14/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.4626 - acc: 0.8233 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 15/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.4538 - acc: 0.8290 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 16/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.4505 - acc: 0.8337 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 17/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.4096 - acc: 0.8432 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 18/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.3725 - acc: 0.8598 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 19/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.3596 - acc: 0.8744 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00Epoch 20/20\n",
      " 78/118 [==================>...........] - ETA: 38s - loss: 0.3542 - acc: 0.8630 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f386a4a4748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=77,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=6,\n",
    "        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    l.trainable = True\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "78/77 [==============================] - 128s 2s/step - loss: 1.7467 - acc: 0.3263 - val_loss: 1.4874 - val_acc: 0.3085\n",
      "Epoch 2/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.5617 - acc: 0.3124 - val_loss: 3.1769 - val_acc: 0.2577\n",
      "Epoch 3/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.4386 - acc: 0.3514 - val_loss: 1.3189 - val_acc: 0.3251\n",
      "Epoch 4/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.4188 - acc: 0.3369 - val_loss: 1.4653 - val_acc: 0.3499\n",
      "Epoch 5/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3982 - acc: 0.3560 - val_loss: 1.2900 - val_acc: 0.4338\n",
      "Epoch 6/20\n",
      "78/77 [==============================] - 110s 1s/step - loss: 1.4065 - acc: 0.3458 - val_loss: 1.4939 - val_acc: 0.3652\n",
      "Epoch 7/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3841 - acc: 0.3516 - val_loss: 1.3298 - val_acc: 0.3333\n",
      "Epoch 8/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3908 - acc: 0.3526 - val_loss: 1.3257 - val_acc: 0.3983\n",
      "Epoch 9/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.4159 - acc: 0.3455 - val_loss: 1.4998 - val_acc: 0.2766\n",
      "Epoch 10/20\n",
      "78/77 [==============================] - 115s 1s/step - loss: 1.4174 - acc: 0.3255 - val_loss: 1.4475 - val_acc: 0.2624\n",
      "Epoch 11/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3983 - acc: 0.3395 - val_loss: 1.4537 - val_acc: 0.2931\n",
      "Epoch 12/20\n",
      "78/77 [==============================] - 110s 1s/step - loss: 1.4429 - acc: 0.3191 - val_loss: 1.3766 - val_acc: 0.3132\n",
      "Epoch 13/20\n",
      "78/77 [==============================] - 115s 1s/step - loss: 1.4064 - acc: 0.3212 - val_loss: 1.4298 - val_acc: 0.2979\n",
      "Epoch 14/20\n",
      "78/77 [==============================] - 115s 1s/step - loss: 1.3817 - acc: 0.3371 - val_loss: 1.3600 - val_acc: 0.3582\n",
      "Epoch 15/20\n",
      "78/77 [==============================] - 115s 1s/step - loss: 1.3640 - acc: 0.3375 - val_loss: 1.3652 - val_acc: 0.3463\n",
      "Epoch 16/20\n",
      "78/77 [==============================] - 115s 1s/step - loss: 1.3587 - acc: 0.3531 - val_loss: 1.3617 - val_acc: 0.3747\n",
      "Epoch 17/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3613 - acc: 0.3448 - val_loss: 1.3160 - val_acc: 0.4125\n",
      "Epoch 18/20\n",
      "78/77 [==============================] - 111s 1s/step - loss: 1.3734 - acc: 0.3292 - val_loss: 1.3332 - val_acc: 0.4019\n",
      "Epoch 19/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3630 - acc: 0.3550 - val_loss: 1.3766 - val_acc: 0.2979\n",
      "Epoch 20/20\n",
      "78/77 [==============================] - 116s 1s/step - loss: 1.3675 - acc: 0.3337 - val_loss: 1.5566 - val_acc: 0.2967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f38684845f8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=77,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=26,\n",
    "        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.556619799447116, 0.29669030739905988]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator,steps=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>photo_553054.jpg</td>\n",
       "      <td>Plecoptera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>photo_585610.jpg</td>\n",
       "      <td>Diptera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>photo_9719.jpg</td>\n",
       "      <td>Diptera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>picture4281.jpg</td>\n",
       "      <td>Ephemeroptera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>picture1347.jpg</td>\n",
       "      <td>Diptera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name          order\n",
       "0  photo_553054.jpg     Plecoptera\n",
       "1  photo_585610.jpg        Diptera\n",
       "2    photo_9719.jpg        Diptera\n",
       "3   picture4281.jpg  Ephemeroptera\n",
       "4   picture1347.jpg        Diptera"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/test/xy.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat = np_utils.to_categorical([t[i] for i in df.order], 4)\n",
    "y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "iml = []\n",
    "for f,o in zip(df.file_name,df.order):\n",
    "    i_path = '../data/test/{}/{}'.format(o,f)\n",
    "    iml.append(cv2.resize(cv2.imread(i_path,1),(224,224),interpolation = cv2.INTER_AREA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iml = np.array(iml)\n",
    "y_cat = np.array(y_cat)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "counter = 0\n",
    "# for xbatch, ybatch in test_datagen.flow(iml,y_cat, batch_size = 32):\n",
    "#     y_true.append(ybatch)\n",
    "#     y_pred.append(model.predict(xbatch))\n",
    "#     if counter == 16:\n",
    "#         break\n",
    "#     counter += 1\n",
    "\n",
    "iml = np.stack(iml)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow(iml,y_cat,\n",
    "        batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(iml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v: k for k, v in t.items()}\n",
    "y_pred = [inv_map[np.argmax(i)] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [inv_map[np.argmax(i)] for i in y_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Diptera       0.75      0.91      0.82       116\n",
      "Ephemeroptera       0.99      0.67      0.80       104\n",
      "   Plecoptera       0.82      0.79      0.81        82\n",
      "  Trichoptera       0.70      0.79      0.75        78\n",
      "\n",
      "  avg / total       0.82      0.80      0.80       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "_____________\n",
    "\n",
    "From RUN TWO\n",
    "_____________\n",
    "'''\n",
    "\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Diptera       0.79      0.91      0.85       113\n",
      "Ephemeroptera       0.93      0.86      0.89       214\n",
      "   Plecoptera       0.73      0.92      0.81        83\n",
      "  Trichoptera       0.92      0.73      0.82       100\n",
      "\n",
      "  avg / total       0.87      0.85      0.85       510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "_____________\n",
    "\n",
    "From RUN ONE\n",
    "_____________\n",
    "'''\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "vgg2 = vgg16.VGG16(include_top=False,weights='imagenet',classes=4,input_tensor=Input((224,224,3)))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 = vgg16.VGG16(weights='imagenet', include_top=False, input_tensor=Input((224, 224, 3)))\n",
    "# print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in vgg2.layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten(input_shape=vgg2.output.shape)(vgg2.output)\n",
    "x = Dense(4096, activation='relu', name='ft_fc1')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(2048, activation='relu', name='ft_fc2')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(4, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "ft_fc1 (Dense)               (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "ft_fc2 (Dense)               (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 8196      \n",
      "=================================================================\n",
      "Total params: 125,886,276\n",
      "Trainable params: 111,167,492\n",
      "Non-trainable params: 14,718,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(inputs=vgg2.input, outputs=predictions)\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1506 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../data/train2',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32, #changed from 1, change back if needed\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 502 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '../data/validation2',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 224 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        '../data/test2',\n",
    "        target_size=(224,224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have shape (None, 4) but got array with shape (32, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f9372c8dc989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2094\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2095\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1806\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1409\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1412\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1413\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (None, 4) but got array with shape (32, 5)"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=118,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=6,\n",
    "        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    l.trainable = True\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=118,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=6,\n",
    "        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_model(dense_layer_sizes, dropout_sizes):\n",
    "#     '''Creates model comprised of 2 convolutional layers followed by dense layers\n",
    "\n",
    "#     dense_layer_sizes: List of layer sizes. This list has one number for each layer\n",
    "#     nb_filters: Number of convolutional filters in each convolutional layer\n",
    "#     nb_conv: Convolutional kernel size\n",
    "#     nb_pool: Size of pooling area for max pooling\n",
    "#     '''\n",
    "    \n",
    "#     x = Flatten(input_shape=vgg16.output.shape)(vgg16.output)\n",
    "#     x = Dense(4096, activation='relu', name='ft_fc1')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(2048, activation='relu', name='ft_fc12')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     predictions = Dense(4, activation = 'softmax')(x)\n",
    "    \n",
    "#     for layer_size, d in dense_layer_sizes, dropout_sizes:\n",
    "#         x = Dense(layer_size, activation = 'relu')(x)\n",
    "#         x = Dropout(d)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     predictions = Dense(4, activation= 'softmax')(x)\n",
    "#     model = Model(inputs=vgg16.input, outputs=predictions)\n",
    "\n",
    "    \n",
    "# #     model = Sequential()\n",
    "\n",
    "# #     model.add(Conv2D(filters, (kernel_size, kernel_size),\n",
    "# #                      padding='valid', input_shape=input_shape))\n",
    "# #     model.add(Activation('relu'))\n",
    "# #     model.add(Conv2D(filters, (kernel_size, kernel_size)))\n",
    "# #     model.add(Activation('relu'))\n",
    "# #     model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "# #     model.add(Dropout(0.25))\n",
    "\n",
    "# #     model.add(Flatten())\n",
    "# #     for layer_size in dense_layer_sizes:\n",
    "# #         model.add(Dense(layer_size))\n",
    "# #         model.add(Activation('relu'))\n",
    "# #     model.add(Dropout(0.5))\n",
    "# #     model.add(Dense(nb_classes))\n",
    "# #     model.add(Activation('softmax'))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adadelta',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_size_candidates = [[512,512],[1024,512],[1024,1024],[2048,1024]]\n",
    "dropout_sizes = [[.5,.5],[.5,.25],[.25,.5],[.75,.5]]\n",
    "my_classifier = KerasClassifier(make_model, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = GridSearchCV(my_classifier,\n",
    "                        param_grid={'dense_layer_sizes':dense_size_candidates,\n",
    "                                    'dropout_sizes':dropout_sizes},\n",
    "                         scoring='f1_samples',\n",
    "                         n_jobs=-1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validator.fit(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=118,\n",
    "#         epochs=50,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=10,\n",
    "#         use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
